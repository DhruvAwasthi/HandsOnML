{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With thousands or millions of features for each training instance, not only training becomes extremely slow but \n",
    "# it also makes it much harder to find a good solution. This is called Curse of Dimensionality. \n",
    "# High dimensional datasets are at risk of being sparse, leading to overfitting many times. So we need to reduce\n",
    "# the dimension of training instances. The process of reducing high-dimensional data into a lower-dimensional data\n",
    "# is called Dimensionality Reduction. It is useful in many cases:\n",
    "# 1. To compress the data so it takes up less computer memory/disk space.\n",
    "# 2. To reduce the dimensions of input data so as to speed up a learning algorithm.\n",
    "# 3. TO visualize high-dimensional data.\n",
    "# There are many techniques to Dimensionaly Reduction e.g., projection, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECTION:\n",
    "# In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features\n",
    "# are almose constant, while others are highly correlated. As a result, all training instances actually lie within\n",
    "# a much lower-dimensional subspace of the high-dimensional space. For e.g, while reducing 3D to 2D, we project \n",
    "# every training instance of 3D perpendicularly onto a subspace (i.e., plane) which results into a 2D dataset.\n",
    "# However, projection is not always the best approach to dimensionality reduction as in many cases the subspace may\n",
    "# twist and turn, such as in the famous Swiss roll toy dataset. Simply projecting onto a plane would squash \n",
    "# different layers of the Swiss roll together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Swiss roll is a example of a 2D manifold. A 2D manifold is a shape that can be bent and twisted in a higher \n",
    "# dimensional space. More generally, a d-dimensional manifold is a part of an n-dimensional space that locally \n",
    "# represents a d-dimensional hyperplane. In the case of swiss roll, d = 2 and n = 3.\n",
    "# Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this \n",
    "# is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which \n",
    "# holds that most real-worlds high-dimensional datasets lie close to a much lower-dimensional manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA) is the most popular dimensionality reduction algorithm. \n",
    "# The unit vector that defines the ith axis is called the ith principal component (PC), which are represented as\n",
    "# c1, c2, c3, ... so on.\n",
    "# BUILDING 3D DATASET\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code uses NumPy's svd() function to obtain all the principal componets of the training set, then \n",
    "# extracts the first two PCs.\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to \n",
    "# d dimensions by projecting it onto the hyperplane defined by the first d principal components. \n",
    "# The following code projects the training set onto the plane defined by the first two principal components:\n",
    "\n",
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Scikit-Learn's PCA class (which implements PCA using SVD decomposition) we can do this as:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.93636116, -0.29854881, -0.18465208],\n",
       "       [ 0.34027485, -0.90119108, -0.2684542 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can access the principal components using the components_ variable (note it contains the PCs as horizontal \n",
    "# vectors)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explained variance ratio indicates the proportion of the dataset's variance that lies alon gthe axis of each \n",
    "# principal component. We can access this via explained_variance_ratio_ variable\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "# This tells you that 84.2% of the dataset's variance lies along the first axis, and 14.6% lies along the second \n",
    "# axis. This leaves less than 1.2% for the third axis, so it is reasonable to assume that it probably carries \n",
    "# little information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate the X_train on MNIST dataset.\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.int64)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose\n",
    "# the dimensions that add up to a sufficiently large portion of the variance. \n",
    "# Following code computes PCA without reducing dimensionlity, then computes the minimum number of dimensions \n",
    "# required to preserve 95% of the training set's variance.\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) +1\n",
    "\n",
    "# You can then set n_components=d and run PCA again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, there is a much better option: instead of specifying the number of principal components you want to \n",
    "# preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ration of variance you wish\n",
    "# to preserve:\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 154)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape\n",
    "# Now after dimensionality reduction, the training set takes uo much less space, and each instance have just over \n",
    "# 150 features, instead of the original 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse \n",
    "# transformation of the PCA projection. But it won't give back the original data, since the projection lost a bit \n",
    "# of information (within the 5% variance that was dropped), but it will likely be quite close to the original data.\n",
    "# The mean squared distance between the original data and the reconstructed data (compressed and then decompressed)\n",
    "# is called the reconstruction error.\n",
    "\n",
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One problem with the preceding implementation of PCA is that it requires the whole training set to fit in memory\n",
    "# in order for the SVD algorithm. To overcome this, we can split the training set into mini-batches and feed an \n",
    "# Incremental PCA (IPCA) algorithm one mini-batch at a time. This is useful for large training sets, and also to \n",
    "# apply PCA online (i.e., on the fly, as new instances arrive).\n",
    "# The following code splits the MNIST dataset into 100 mini-batches and feed them to Scikit-Learn's IncrementalPCA\n",
    "# class to reduce the dimensionality of the MNIST dataset down to 154 dimensions. Now we must call partial_fir() \n",
    "# method instead of fit() method:\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use NumPy's memmap class, which allows you to manipulate a large array stored in a binary\n",
    "# file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it need it.\n",
    "# Since the IncrementalPCA class uses only a small part of the array at any given time, the memory usage remains \n",
    "# under control\n",
    "#\n",
    "# X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "#\n",
    "# batche_size = m // n_batches\n",
    "# inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "# inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn offers another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that \n",
    "# quickly finds as approximation of the first d principal components. Its computational complexity is:\n",
    "# O(mXn^2) + O(d^3), instead of O(mXn^2). + O(n^3)\n",
    "\n",
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel trick maps instances into a very high-dimensional space (called the featuere space), enabling nonlinear\n",
    "# classification and regression with Support Vector Machines. The same trick can be applied to PCA, making it \n",
    "# possible to perform complex nonlinear projections for dimensionality reducitons. This is called Kernel PCA (kPCA)\n",
    "# It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that\n",
    "# lie close to a twisted manifold.\n",
    "#\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "#\n",
    "# rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "# X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As kPCA is an insupervised learning algorithm, there is no obvious performance measure to help you select the \n",
    "# best kernel and hyperparameter values. However, dimensionality reduction is often a preparation step for a \n",
    "# supervised learning task, so you can simply use grid search to select the kernel and hyperparameters that lead to\n",
    "# the best performance on that task.\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# clf = Pipeline([\n",
    "#     (\"kpca\", KernelPCA(n_components=2)),\n",
    "#     (\"log_reg\", LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# param_grid = [{\n",
    "#    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "#    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "# }]\n",
    "\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# The best kernel and hyperparameters are then available throught the best_params_  variable:\n",
    "# print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another approach, this time entirely unsupervised, is to select the kernel and hyperparameters that yield the \n",
    "# lowest reconstruction error. However, reconstruction is not as easy as with linear PCA.  \n",
    "# For e.g., let's imagine the original Swiss roll 3D dataset and the resulting 2D dataset after kPCA is applied \n",
    "# using an RBF kernel. Thanks to the kernel trick, this is mathematically equivalent to mapping the training set\n",
    "# to an infinite-dimensional feature space using the feature map, then projecting the transformed training set down\n",
    "# to 2D using linear PCA. Notice that if we could invert the linear PCA step for a given instance in the reduced \n",
    "# space, the reconstructed point would lie in feature space, not in the original space. Since the feature space is\n",
    "# infinite-dimensional, we cannot compute the reconstructed point, and therefore we cannot compute the true \n",
    "# reconstruction error.\n",
    "# Fortunately it is possible to a point in the original space that would map close to the reconstructed point.\n",
    "# This is called reconstruction pre-image. Once you have this pre-image, you can measure its squared distance to\n",
    "# the original instance. You can then select the kernel and hyperparamters that minimize this reconstruction pre-\n",
    "# image error.\n",
    "\n",
    "# Now to perform this reconstruction, one solution is to train a supervised regression model, with the projected\n",
    "# instances as the training set and the original instances as the targets. Scikit-Learn will do this automatically\n",
    "# if you set fit_inverse_transform=True, as shown in following code:\n",
    "\n",
    "# rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "# X_reduced = rbf_pca.fit_transform(X)\n",
    "# X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "# You can then compute the reconstruction pre-image error:\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# mean_squared_error(X, X_preimage)\n",
    "# evaluates to 32.786308795766132\n",
    "\n",
    "# Now you can use grid search with cross-validation to find the kernel and hyperparameters that minimize this \n",
    "# pre-image reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Linear Embedding (LLE) is another very powerful nonlinear dimensionality reduction (NLDR) technique. It is \n",
    "# a Manifold Learning technique that does not rely on projections like the previous algorithms. \n",
    "# In a nutshell, LLE works by first measuring how each training instance linearly relates to its closest neighbors\n",
    "# (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships\n",
    "# are best preserved. This makes it particularly good at unrolling manifolds, especially when there is not too much\n",
    "# noise.\n",
    "\n",
    "# The following code uses Scikit-Learn's LocallyLinearEmbedding class to unroll the Swiss roll.\n",
    "\n",
    "# from sklearn.manifold import LocallyLinearEmbedding\n",
    "#\n",
    "# lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "# X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first 60,000 instances for training, and the remaining 10,000 for testing\n",
    "X_train = mnist['data'][:60000]\n",
    "y_train = mnist['target'][:60000]\n",
    "\n",
    "X_test = mnist['data'][60000:]\n",
    "y_test = mnist['target'][60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest classifier on the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 4.48s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 10.67s\n"
     ]
    }
   ],
   "source": [
    "# Now let's train a new radom forest classifier on the reduced dataset.\n",
    "\n",
    "rnd_clf2 = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "t0 = time.time()\n",
    "rnd_clf2.fit(X_train_reduced, y_train)\n",
    "t1 = time.time()\n",
    "print(\"Training took {:.2f}s\".format(t1 - t0))\n",
    "\n",
    "# Oh no! Training is actually more than twice slower now! How can that be? Well, as we saw in this chapter, \n",
    "# dimensionality reduction does not always lead to faster training time: it depends on the dataset, the model and \n",
    "# the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9009"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's evaluate the classifier on the test set\n",
    "\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "y_pred = rnd_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# It is common for performance to drop slightly when reducing dimensionality, because we do lose some useful \n",
    "# signal in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caesar/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Now let's see if softmax regression helps\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
    "t0 = time.time()\n",
    "log_clf.fit(X_train, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 14.49s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9255"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating scores\n",
    "y_pred = log_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caesar/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Okay, so softmax regression takes much longer to train on this dataset than the random forest classifier, plus \n",
    "# it performs worse on the test set. But that's not what we are interested in right now, we want to see how much\n",
    "# PCA can help softmax regression. Let's train the softmax regression model using the reduced dataset:\n",
    "\n",
    "log_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
    "t0 = time.time()\n",
    "log_clf2.fit(X_train_reduced, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 5.39s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))\n",
    "\n",
    "# Nice! Reducing dimensionality led to a 3× speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9201"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's evaluate the accuracy score:\n",
    "\n",
    "y_pred = log_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
