{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78wtGSinCMu1"
   },
   "source": [
    "# Installing and Importing TensorFlow v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "_Z7UWJa5yYuq",
    "outputId": "daca424c-144e-46c4-d599-73b67d22cc32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.14.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.18.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (3.11.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.11.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.9.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.34.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.3.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.27.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/caesar/anaconda3/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.0)\n",
      "Requirement already satisfied: h5py in /home/caesar/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TensorFlow version: 1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/caesar/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import the TensorFlow and output the verion\n",
    "\n",
    "!pip install tensorflow==1.14.0\n",
    "import tensorflow as tf\n",
    "print(\"\\n\\nTensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rabMUOVb_e2H"
   },
   "source": [
    "# Creating Your First Graph and Running It in a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6HKrm7pzfAX"
   },
   "outputs": [],
   "source": [
    "# The following code creates a computation graph which can be used to do the computation later on\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "2EMlF1CazjlA",
    "outputId": "109d2abb-b468-4c7a-a139-94599f7ce655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# The following code creates a session, initilizes the variables, and evaluates f then closes the session.\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9jLBYTKzl5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Having to repeat sess.run() all the time is a bit cumbersome, so there is a better way to do this:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  x.initializer.run()         # it is similat to tf.get_default_session().run(x.initializer)\n",
    "  y.initializer.run()         # it is similar to tf.get_deafult_session().run(y.initializer)\n",
    "  result = f.eval()           # it is similar to tf.get_default_session().run(f)\n",
    "  print(result)\n",
    "\n",
    "# Inside the with block, the session is set as the default session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvFNX-M61elw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'add_1:0' shape=() dtype=int32>>\n"
     ]
    }
   ],
   "source": [
    "# Instead of manually running the initializer for every single variable, you can use the \n",
    "# global_variables_initializer() function that creates a node in the graph that will initialize all the variables \n",
    "# when it runs\n",
    "\n",
    "init = tf.global_variables_initializer()      # prepare an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  init.run() # actually initializes all the variables\n",
    "  result = f.eval\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TF9znF6s6K9n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# We can also use and InteractiveSession which differs from a regular Session in the fact that when an \n",
    "# InteractiveSession is created it automatically sets itself as the default session, so you don't need a with \n",
    "# block.\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3VA43rUAA2e"
   },
   "source": [
    "**NOTE:** A TensorFlow program is typically split into two parts: the first part builds a computation graph (this is called the *construction* phase) that represents the ML model and the computations required to train it, an the second part runs it (this is called the *execution* phase) that generally runs a loop that evaluates a training step repeatedly, gradually improving the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fq1rhi7TARD7"
   },
   "source": [
    "# Managing Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhf7qL6pAS5r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any node you create is automatically added to the default graph:\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2k-Go6BUBW2g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In most cases it is fine, but sometimes you may want to manage multiple independent graphs. You can do this by \n",
    "# creating a new Graph and temporarily making it the default graph inside a with block:\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  x2 = tf.Variable(2)\n",
    "\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NvLIy8AB0sA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWrke8SdB_5C"
   },
   "source": [
    "# Lifecycle of a Node Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ll1zkqkcB4WL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 10\n",
      "z: 8\n"
     ]
    }
   ],
   "source": [
    "# A variable starts its life when its initializer is run, and it ends when the session is closed. \n",
    "# When you evaluate a graph, TensorFlow automatically determines the set of nodes that it depends on and it \n",
    "# evaluates these nodes first. For example consider the following code:\n",
    "\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x + 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(\"y:\", y.eval())\n",
    "  print(\"z:\", z.eval())\n",
    "\n",
    "# This starts a session adn runs the graph to evaluate y: TensorFlow automatically detects that y depends on x, \n",
    "# which depends on w, so it first evaluates w, then x, then y, and returns the value of y. Finally, the code runs \n",
    "# the graph to evaluate z. Once again, TensorFlow detects that it must first evaluate w and x. It will not reuse \n",
    "# the result of the previous evaluations of w and x. The code evaluates w and x twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GTyDrptFBpt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 10\n",
      "z: 8\n"
     ]
    }
   ],
   "source": [
    "# If you want to evaluate y and z efficiently, without evaluating w and x twice, you must ask TensorFlow to \n",
    "# evaluate both y and x in just one graph run:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  y_val, z_val = sess.run([y, z])\n",
    "  print(\"y:\", y_val)\n",
    "  print(\"z:\", z_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQSRL4l2H_Mt"
   },
   "source": [
    "# Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6skZU8UkH9HP"
   },
   "outputs": [],
   "source": [
    "# TensorFlow operations are called ops for short. Constants and variables take no input, so they are called \n",
    "# source ops. The inputs and outputs are multidimensional arrays, called tensors (hence the name tensor flow).\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96LE5eP2MJ9C"
   },
   "source": [
    "# Implementing Gradient Descnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzHaGommMP7K"
   },
   "source": [
    "### Manually Computing the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlRLfRq2J5Lz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 12.012333\n",
      "Epoch 100 MSE = 0.86637396\n",
      "Epoch 200 MSE = 0.65275073\n",
      "Epoch 300 MSE = 0.6161807\n",
      "Epoch 400 MSE = 0.5920384\n",
      "Epoch 500 MSE = 0.57444745\n",
      "Epoch 600 MSE = 0.5615484\n",
      "Epoch 700 MSE = 0.55206585\n",
      "Epoch 800 MSE = 0.54507715\n",
      "Epoch 900 MSE = 0.53991216\n",
      "518388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "    sess.run(training_op)\n",
    "  \n",
    "  best_theta = theta.eval()\n",
    "b = datetime.datetime.now()\n",
    "c = b - a\n",
    "print(c.microseconds)\n",
    "\n",
    "# The random_uniform() function creates a node in the graph that will generate a tensor containing random values,\n",
    "# given its shape and value range. The assign() funnction creates a node that will assign a new value to a \n",
    "# variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tfpy6WhqRqCt"
   },
   "source": [
    "### Using autodiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFlpij7lOT-t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.8267274\n",
      "Epoch 100 MSE = 0.81548864\n",
      "Epoch 200 MSE = 0.7099729\n",
      "Epoch 300 MSE = 0.65776706\n",
      "Epoch 400 MSE = 0.6207093\n",
      "Epoch 500 MSE = 0.59397864\n",
      "Epoch 600 MSE = 0.5746767\n",
      "Epoch 700 MSE = 0.56073546\n",
      "Epoch 800 MSE = 0.5506641\n",
      "Epoch 900 MSE = 0.5433867\n",
      "515099\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow's autodiff feature can automatically and efficiently compute the gradeints for you.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "    sess.run(training_op)\n",
    "  \n",
    "  best_theta = theta.eval()\n",
    "b = datetime.datetime.now()\n",
    "c = b - a\n",
    "print(c.microseconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCO7QxH8RclH"
   },
   "source": [
    "## Using an Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MByay2EqQo22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.347848\n",
      "Epoch 100 MSE = 0.7398799\n",
      "Epoch 200 MSE = 0.5951053\n",
      "Epoch 300 MSE = 0.5768822\n",
      "Epoch 400 MSE = 0.5652666\n",
      "Epoch 500 MSE = 0.55645216\n",
      "Epoch 600 MSE = 0.54966515\n",
      "Epoch 700 MSE = 0.54440844\n",
      "Epoch 800 MSE = 0.54031545\n",
      "Epoch 900 MSE = 0.5371121\n",
      "457194\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow provides a number of optimizers to simplify the code. You just need to select the one.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "    sess.run(training_op)\n",
    "  \n",
    "  best_theta = theta.eval()\n",
    "b = datetime.datetime.now()\n",
    "c = b - a\n",
    "print(c.microseconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91BQGrPqT3d-"
   },
   "source": [
    "# Feeding Data to the Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9zZuTxCRgeN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_val_1:\n",
      " [[6. 7. 8.]]\n",
      "\n",
      "B_val_2:\n",
      " [[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "# To pass the training data to TensorFlow during training, we use placeholder nodes which don't actually perform \n",
    "# any computation but just output the data you tell them to output at runtime. For example, the following code \n",
    "# creates a placeholder node A and also a node B. When we evaluate B, we pass a feed_dict to the eval() method \n",
    "# that specifies the value of A.\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "  B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(\"B_val_1:\\n\", B_val_1)\n",
    "print(\"\\nB_val_2:\\n\", B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3LwzjdAWz5w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "# Let's implement Mini-Batch Gradient Descent\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Change the definition of X and y in construction phase to make them placeholder nodes:\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Define the batch size and compute the total number of batches:\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "  np.random.seed(epoch * n_batches + batch_index)\n",
    "  indices = np.random.randint(m, size=batch_size)\n",
    "  X_batch = scaled_housing_data_plus_bias[indices]\n",
    "  y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "  return (X_batch, y_batch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    for batch_index in range(n_batches):\n",
    "      X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "  \n",
    "  best_theta = theta.eval()\n",
    "\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awMjFm5Yr7Vd"
   },
   "source": [
    "# Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VR_EECY_pSbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow makes saving and restoring a model very easy. Just create a Saver node at the end of the construction\n",
    "# phase; then, in the execution phase, just call its save() method whenever you want to save the model, passing it\n",
    "# the session and path of the checkpoint file.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Change the definition of X and y in construction phase to make them placeholder nodes:\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Define the batch size and compute the total number of batches:\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "  np.random.seed(epoch * n_batches + batch_index)\n",
    "  indices = np.random.randint(m, size=batch_size)\n",
    "  X_batch = scaled_housing_data_plus_bias[indices]\n",
    "  y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "  return (X_batch, y_batch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    if epoch % 100 == 0:\n",
    "      save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "    for batch_index in range(n_batches):\n",
    "      X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "      save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "  \n",
    "  best_theta = theta.eval()\n",
    "  save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clO0y611uOLe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "# Now to restore a model at the beginning of the execution phase instead of initializing the vairables using the \n",
    "# init node, you call the restore() method of the Saver object:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "  best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b45gBw50wksQ"
   },
   "source": [
    "By default, the save() method also saves the structure of the graph in a second file with the same name plus a .meta extension. You can load this graph using `tf.train.import_meta_graph()`. This adds the graph to the default graph. \n",
    "\n",
    "---\n",
    "\n",
    "By default, the `Saver` saves and restores all variables under their own name, but if you need more control, you can specify which variables to save or restore, and names to use. For example, the following `Saver` will save or restore only the theta variable under the name `weights`:\n",
    "\n",
    "> saver = tf.train.Saver({\"weights\": theta})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONAZFtOEeDcU"
   },
   "source": [
    "# Visualizing the Graph and Training Curves Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_v9JLAdIeA7Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard display nice visualizations of stats in your web browser (e.g, learning curves). Youc an also \n",
    "# provide it the graph's definition and it will give you a great interface to browse through it. This is very \n",
    "# useful to identify errors in the graph, to find bottlenecks, and so on. You need to write the training error \n",
    "# (MSE) to a log directory that TensorBoard will read from.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Change the definition of X and y in construction phase to make them placeholder nodes:\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Define the batch size and compute the total number of batches:\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# This line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible binary log string called a summary.\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "\n",
    "# This line createa a FileWriter that you will use to write summaries to logfiles in the log directory. \n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "  np.random.seed(epoch * n_batches + batch_index)\n",
    "  indices = np.random.randint(m, size=batch_size)\n",
    "  X_batch = scaled_housing_data_plus_bias[indices]\n",
    "  y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "  return (X_batch, y_batch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    if epoch % 100 == 0:\n",
    "      save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "    for batch_index in range(n_batches):\n",
    "      X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "      \n",
    "      # Update the execution phase to evaluate the mse_summary node regularly during training (e.g., every 10 mini-batches).\n",
    "      if batch_index % 10 == 0:\n",
    "        summary_str = mse_summary.eval(feed_dict= {X: X_batch, y: y_batch})\n",
    "        step = epoch * n_batches + batch_index\n",
    "        file_writer.add_summary(summary_str, step)\n",
    "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "  \n",
    "  best_theta = theta.eval()\n",
    "  save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "  file_writer.close()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6008\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbfe236d290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tf_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "# When dealing wiht more complex models such as neural networks, the computation graph can easily become cluttered\n",
    "# with thousands of nodes. To avoid this, you can create name scopes to group related nodes. For example, let's\n",
    "# modify the previous code to define the error and mse ops within a name scope called \"loss\"\n",
    "\n",
    "# TensorBoard display nice visualizations of stats in your web browser (e.g, learning curves). Youc an also \n",
    "# provide it the graph's definition and it will give you a great interface to browse through it. This is very \n",
    "# useful to identify errors in the graph, to find bottlenecks, and so on. You need to write the training error \n",
    "# (MSE) to a log directory that TensorBoard will read from.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Change the definition of X and y in construction phase to make them placeholder nodes:\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# Define the batch size and compute the total number of batches:\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Define the error and mse ops within a name scope called \"loss\"\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# This line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible \n",
    "# binary log string called a summary.\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "\n",
    "# This line createa a FileWriter that you will use to write summaries to logfiles in the log directory. \n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "  np.random.seed(epoch * n_batches + batch_index)\n",
    "  indices = np.random.randint(m, size=batch_size)\n",
    "  X_batch = scaled_housing_data_plus_bias[indices]\n",
    "  y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "  return (X_batch, y_batch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    if epoch % 100 == 0:\n",
    "      save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "    for batch_index in range(n_batches):\n",
    "      X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "      \n",
    "      # Update the execution phase to evaluate the mse_summary node regularly during training (e.g., every 10 mini-batches).\n",
    "      if batch_index % 10 == 0:\n",
    "        summary_str = mse_summary.eval(feed_dict= {X: X_batch, y: y_batch})\n",
    "        step = epoch * n_batches + batch_index\n",
    "        file_writer.add_summary(summary_str, step)\n",
    "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "  \n",
    "  best_theta = theta.eval()\n",
    "  save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "  file_writer.close()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n",
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "# The name of each op defined within the scope is now prefixed with \"loss/\"\n",
    "print(error.op.name)\n",
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 22637), started 0:01:29 ago. (Use '!kill 22637' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6008\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc050560f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In TensorBoard, the mse and error nodes now appear inside the loss namespace, which appears collapse by default\n",
    "%tensorboard --logdir tf_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you want to create a graph that adds the output of two rectified liear units (ReLU). A ReLU computes a \n",
    "# linear fucntion of the inputs, and outputs the result if it is positive, and 0 otherwise.\n",
    "# The following code do the job but its repetitive:\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z2, 0., name=\"relu2\")\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Such repetitive code is hard to maintain and error-prone. TensorFlow lets you stay DRY (Don't Repeat Yourself):\n",
    "# simply create a function to build ReLu. The following code creates five ReLUs and outputs their sum:\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())\n",
    "# add_n() creates an operation that will compute the sum of a list of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** TensorFlow checks whether its name already exists, and if it does it appends an underscore followed by\n",
    "an index to make the name unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbfe1dcbd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using name scopes, you can make the graph much clearer. \n",
    "\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 22749), started 0:00:00 ago. (Use '!kill 22749' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0174ecc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you want to control the ReLU threshold for all the ReLUs using a shared threshold variable for all ReLUs.\n",
    "# The following code first defines the relu() function, then creates the relu/threshold variable (as a scalar that\n",
    "# will later be initialized to 0.0) and builds five ReLUs by calling the relu() function. The relu() function \n",
    "# reuses the relu/threshold variable, and creates the other ReLU nodes.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")    # reuse existing variable\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu3\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 22749), started 0:00:00 ago. (Use '!kill 22749' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbfe19e8750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is somewhat unfortunate that the threshold variable must be defined outside the relu() function, where all the\n",
    "# rest of the ReLU code resides. To fix this, the following ccode creates the threshold variable wihtin the relu()\n",
    "# function upon the first call, then reuses it in subsequent calls. \n",
    "# Now the relu() function does not have to worry about name scopes or variable sharing: it just calls \n",
    "# get_variable(), which will create or reuse the threshold variable. The rest of the code calls relu() five times,\n",
    "# making sure to set reuse=False on the first call, and reuse=True for the other calls.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))   \n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, threshold, name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu4\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 22749), started 0:00:00 ago. (Use '!kill 22749' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc050549150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Logistic Regression with Mini-Batch Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create the dataset\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's take a peek at the dataset:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label='Positive')\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label='Negative')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a bias feature\n",
    "import numpy as np\n",
    "\n",
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]\n",
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the dataset\n",
    "\n",
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Let's build a model\n",
    "n_inputs = 2 # The moons dataset have two features\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = tf.sigmoid(logits)\n",
    "loss = tf.losses.log_loss(y, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the optimizer and tell it to minimize the loss:\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tloss: 1.02737\n",
      "Epoch: 100 \tloss: 0.3102982\n",
      "Epoch: 200 \tloss: 0.27445376\n",
      "Epoch: 300 \tloss: 0.21232982\n",
      "Epoch: 400 \tloss: 0.23607779\n",
      "Epoch: 500 \tloss: 0.16002956\n",
      "Epoch: 600 \tloss: 0.3600346\n",
      "Epoch: 700 \tloss: 0.26147836\n",
      "Epoch: 800 \tloss: 0.25837082\n",
      "Epoch: 900 \tloss: 0.24153747\n"
     ]
    }
   ],
   "source": [
    "# Let's run the model:\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tloss:\", loss_val)\n",
    "    \n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8613861386138614"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compute model's precision and recall\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_pred = (y_proba_val >= 0.5)\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8787878787878788"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.8140705\n",
      "Epoch: 500 \tLoss: 0.17466179\n",
      "Epoch: 1000 \tLoss: 0.1261965\n",
      "Epoch: 1500 \tLoss: 0.10195139\n",
      "Epoch: 2000 \tLoss: 0.08693672\n",
      "Epoch: 2500 \tLoss: 0.076758385\n",
      "Epoch: 3000 \tLoss: 0.06931684\n",
      "Epoch: 3500 \tLoss: 0.06367952\n",
      "Epoch: 4000 \tLoss: 0.0591808\n",
      "Epoch: 4500 \tLoss: 0.05553648\n",
      "Epoch: 5000 \tLoss: 0.052499924\n",
      "Epoch: 5500 \tLoss: 0.049958106\n",
      "Epoch: 6000 \tLoss: 0.04776357\n",
      "Epoch: 6500 \tLoss: 0.04585248\n",
      "Epoch: 7000 \tLoss: 0.044185482\n",
      "Epoch: 7500 \tLoss: 0.042698577\n",
      "Epoch: 8000 \tLoss: 0.041363414\n",
      "Epoch: 8500 \tLoss: 0.040174454\n",
      "Epoch: 9000 \tLoss: 0.039082114\n",
      "Epoch: 9500 \tLoss: 0.038097635\n",
      "Epoch: 10000 \tLoss: 0.037230458\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"./my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797979797979798"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797979797979798"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "78wtGSinCMu1",
    "rabMUOVb_e2H",
    "Fq1rhi7TARD7",
    "dWrke8SdB_5C",
    "cQSRL4l2H_Mt",
    "96LE5eP2MJ9C",
    "91BQGrPqT3d-"
   ],
   "name": "chapter_9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
